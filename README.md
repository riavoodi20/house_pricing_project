# house_pricing_project

# House Price Prediction with XGBoost

Predicting house prices using the Ames Housing Dataset with machine learning. Achieves strong performance through feature engineering and XGBoost optimization.

## Results
- **Model:** XGBoost Regressor
- **Test RÂ²:** 0.XXX
- **Test RMSE:** $XX,XXX
- **Features:** 17 engineered features from 79 original variables

## ğŸ› ï¸ Tech Stack
Python â€¢ pandas â€¢ scikit-learn â€¢ XGBoost â€¢ matplotlib

## ğŸ“Š Approach

**Data Processing:**
- Handled missing values with domain knowledge (e.g., 'None' for missing garage = no garage)
- Created age features (HouseAge, YearsSinceRemodel)
- Combined area measurements (TotalSF = 1st + 2nd + Basement)
- Converted quality ratings to numerical scales

**Model Selection:**
Compared Linear Regression, Random Forest, Gradient Boosting, and XGBoost. XGBoost performed best with heavy regularization to prevent overfitting.

**Key Features:**
TotalSF â€¢ OverallQual â€¢ GrLivArea â€¢ HouseAge â€¢ ExterQual_num â€¢ TotalBath â€¢ GarageCars

## ğŸš€ Usage

```python
import xgboost as xgb
import pandas as pd

# Load processed data
df = pd.read_csv('data/df_model_ready.csv')
X = df.drop(columns=['SalePrice'])

# Train model
model = xgb.XGBRegressor(
    n_estimators=1000, learning_rate=0.005, max_depth=2,
    reg_alpha=25, reg_lambda=25, random_state=42
)
model.fit(X_train, y_train)
```

## ğŸ“ Structure
```
â”œâ”€â”€ data/
â”‚   â””â”€â”€ df_model_ready.csv    # Processed dataset
â”œâ”€â”€ 01_eda_preprocessing.py   # Data exploration & cleaning
â”œâ”€â”€ 02_feature_engineering.py # Feature creation
â”œâ”€â”€ 03_model_comparison.py    # Algorithm comparison
â””â”€â”€ 04_final_xgboost.py      # Optimized final model
```

---
**Dataset:** [Ames Housing (Kaggle)](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) â€¢ 1,460 samples â€¢ 79 original features
